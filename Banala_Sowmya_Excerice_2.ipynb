{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5_oFVd9-pY"
      },
      "source": [
        "## The second In-class-exercise (09/13/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
        "Execute all the cells before your final submission."
      ],
      "metadata": {
        "id": "mAzh1U0sE5I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
      ],
      "metadata": {
        "id": "PpgvZQdRE-HV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBZI-je9-pZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-LmNR3kw9-pa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "479c9dad-2089-4467-9864-73fe102755f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nWhat potential effects on urban planning and public health efforts can the use of urban green space have on inhabitants in various demographic groups in terms of their mental health?\\n\\nA complete dataset made up of a range of data kinds should be gathered in order to provide an answer to this research topic. The objective is to comprehend the connection between urban green areas and mental health while taking into account the potential impact of demographic factors. \\nThe steps for gathering and storing the data are as follows:\\n\\nSpecify the variables:\\n\\nUrban green space variables include their location, size, kind (such as parks or gardens), accessibility, and quality. The mapping of green spaces can be done using Geographic Information System (GIS) data.\\nGather data on the age, gender, socioeconomic situation, level of education, and ethnicity of the population.\\nVariables Affecting Mental Health: Use standardized mental health assessment measures (such the PHQ-9 or GAD-7) to gauge your level of stress, anxiety, and depression as well as your general well-being.\\nInclude information on temperature, noise levels, and air quality to take into consideration any potential complicating factors.\\n\\nSampling Plan: Utilize statistical power estimates to select a meaningful sample size. To guarantee an acceptable representation of diverse demographic groups within the study area, a stratified random sampling strategy may be utilized.\\n\\nData Gathering Techniques:\\n\\nSurveys should be given out to inhabitants in order to get information on their demographics and mental health. To ensure diversity in the sample, use stratified sampling.\\nGIS Data: Obtain spatial data about green areas from regional or local government organizations or publicly available databases.\\nEnvironmental Information: Work with pertinent organizations to gather information on the research area's temperature, noise level, and air quality.\\n\\nData Retention:\\n\\nTo keep the data gathered, create a safe and orderly database. For flexibility and scalability, use a relational database system (like MySQL, PostgreSQL) or NoSQL databases (like MongoDB).\\nWhen managing sensitive information, make sure to adhere to data protection laws (such as GDPR and HIPAA).\\nData Preprocessing and Cleaning\\n\\nThe acquired data should be cleaned and preprocessed, managing missing values, outliers, and inconsistent data.\\nTo connect demographic information with geographic places, geocode resident addresses.\\nData Evaluation\\n\\nExamine the association between green space variables, demographic factors, and mental well-being ratings using statistical analysis, including regression models.\\nExamine cutting-edge methods like spatial analysis to investigate spatial patterns and clustering effects\\n\\nEthics-Related Matters:\\n\\nEnsure survey participants' privacy and anonymity and obtain their informed consent.\\nProtect private information and follow moral standards when conducting research on human participants.\\nVisualizing data\\n\\nTo effectively communicate findings, create educational data visualizations such as maps, charts, and graphs.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\"\"\"\n",
        "\n",
        "What potential effects on urban planning and public health efforts can the use of urban green space have on inhabitants in various demographic groups in terms of their mental health?\n",
        "\n",
        "A complete dataset made up of a range of data kinds should be gathered in order to provide an answer to this research topic. The objective is to comprehend the connection between urban green areas and mental health while taking into account the potential impact of demographic factors.\n",
        "The steps for gathering and storing the data are as follows:\n",
        "\n",
        "Specify the variables:\n",
        "\n",
        "Urban green space variables include their location, size, kind (such as parks or gardens), accessibility, and quality. The mapping of green spaces can be done using Geographic Information System (GIS) data.\n",
        "Gather data on the age, gender, socioeconomic situation, level of education, and ethnicity of the population.\n",
        "Variables Affecting Mental Health: Use standardized mental health assessment measures (such the PHQ-9 or GAD-7) to gauge your level of stress, anxiety, and depression as well as your general well-being.\n",
        "Include information on temperature, noise levels, and air quality to take into consideration any potential complicating factors.\n",
        "\n",
        "Sampling Plan: Utilize statistical power estimates to select a meaningful sample size. To guarantee an acceptable representation of diverse demographic groups within the study area, a stratified random sampling strategy may be utilized.\n",
        "\n",
        "Data Gathering Techniques:\n",
        "\n",
        "Surveys should be given out to inhabitants in order to get information on their demographics and mental health. To ensure diversity in the sample, use stratified sampling.\n",
        "GIS Data: Obtain spatial data about green areas from regional or local government organizations or publicly available databases.\n",
        "Environmental Information: Work with pertinent organizations to gather information on the research area's temperature, noise level, and air quality.\n",
        "\n",
        "Data Retention:\n",
        "\n",
        "To keep the data gathered, create a safe and orderly database. For flexibility and scalability, use a relational database system (like MySQL, PostgreSQL) or NoSQL databases (like MongoDB).\n",
        "When managing sensitive information, make sure to adhere to data protection laws (such as GDPR and HIPAA).\n",
        "Data Preprocessing and Cleaning\n",
        "\n",
        "The acquired data should be cleaned and preprocessed, managing missing values, outliers, and inconsistent data.\n",
        "To connect demographic information with geographic places, geocode resident addresses.\n",
        "Data Evaluation\n",
        "\n",
        "Examine the association between green space variables, demographic factors, and mental well-being ratings using statistical analysis, including regression models.\n",
        "Examine cutting-edge methods like spatial analysis to investigate spatial patterns and clustering effects\n",
        "\n",
        "Ethics-Related Matters:\n",
        "\n",
        "Ensure survey participants' privacy and anonymity and obtain their informed consent.\n",
        "Protect private information and follow moral standards when conducting research on human participants.\n",
        "Visualizing data\n",
        "\n",
        "To effectively communicate findings, create educational data visualizations such as maps, charts, and graphs.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QpWOgjHi9-pa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Simulated demographic data\n",
        "def generate_demographic_data(num_samples):\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        age = random.randint(18, 70)\n",
        "        gender = random.choice([\"Male\", \"Female\", \"Other\"])\n",
        "        socioeconomic_status = random.choice([\"Low\", \"Medium\", \"High\"])\n",
        "        education_level = random.choice([\"High School\", \"Bachelor's\", \"Master's\", \"PhD\"])\n",
        "        ethnicity = random.choice([\"White\", \"Black\", \"Asian\", \"Hispanic\", \"Other\"])\n",
        "        data.append([age, gender, socioeconomic_status, education_level, ethnicity])\n",
        "    return data\n",
        "\n",
        "# Simulated mental well-being data\n",
        "def generate_mental_wellbeing_data(num_samples):\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        depression_score = random.randint(0, 30)\n",
        "        anxiety_score = random.randint(0, 30)\n",
        "        stress_score = random.randint(0, 30)\n",
        "        overall_wellbeing_score = random.randint(0, 100)\n",
        "        data.append([depression_score, anxiety_score, stress_score, overall_wellbeing_score])\n",
        "    return data\n",
        "\n",
        "# Generate 1000 samples of demographic and mental well-being data\n",
        "num_samples = 1000\n",
        "demographic_data = generate_demographic_data(num_samples)\n",
        "mental_wellbeing_data = generate_mental_wellbeing_data(num_samples)\n",
        "\n",
        "# Create a DataFrame to store the data\n",
        "columns_demographic = [\"Age\", \"Gender\", \"Socioeconomic Status\", \"Education Level\", \"Ethnicity\"]\n",
        "columns_mental_wellbeing = [\"Depression Score\", \"Anxiety Score\", \"Stress Score\", \"Overall Well-being Score\"]\n",
        "\n",
        "demographic_df = pd.DataFrame(demographic_data, columns=columns_demographic)\n",
        "mental_wellbeing_df = pd.DataFrame(mental_wellbeing_data, columns=columns_mental_wellbeing)\n",
        "\n",
        "# Combine demographic and mental well-being data\n",
        "combined_df = pd.concat([demographic_df, mental_wellbeing_df], axis=1)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "combined_df.to_csv(\"sample_data.csv\", index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P5rjlclf9-pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118271b3-64e8-464a-83d2-1531af8a3be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 0 articles and saved to 'articles.json'.\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# Function to fetch articles from Google Scholar\n",
        "def fetch_google_scholar_articles(query, start_year, end_year, num_articles):\n",
        "    # Base URL for Google Scholar\n",
        "    url = \"https://scholar.google.com/scholar\"\n",
        "    articles = []  # List to store the collected articles\n",
        "\n",
        "    # Loop to paginate through search results (10 results per page)\n",
        "    for start in range(0, num_articles, 10):\n",
        "        # Parameters for the search query, including keywords and date range\n",
        "        params = {\n",
        "            \"q\": query,             # Search query\n",
        "            \"as_ylo\": start_year,   # Start year of publication range\n",
        "            \"as_yhi\": end_year,     # End year of publication range\n",
        "            \"start\": start          # Pagination offset\n",
        "        }\n",
        "\n",
        "        # User-Agent header to mimic a web browser\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.1234.0 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "        # Send a GET request to the Google Scholar search URL with parameters and headers\n",
        "        response = requests.get(url, params=params, headers=headers)\n",
        "\n",
        "        # Check if the response is successful (HTTP status code 200)\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content of the response using BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all the search result div elements\n",
        "            results = soup.find_all('div', {'class': 'gs_ri'})\n",
        "\n",
        "            # Iterate through each search result\n",
        "            for result in results:\n",
        "                article = {}  # Dictionary to store article information\n",
        "\n",
        "                # Extract the title of the article (inside an h3 element with class 'gs_rt')\n",
        "                title = result.find('h3', {'class': 'gs_rt'})\n",
        "                if title:\n",
        "                    article['title'] = title.text  # Store the title in the dictionary\n",
        "\n",
        "                # Extract the venue/journal/conference information (inside a div element with class 'gs_a')\n",
        "                venue = result.find('div', {'class': 'gs_a'})\n",
        "                if venue:\n",
        "                    article['venue'] = venue.text  # Store the venue information\n",
        "\n",
        "                # Extract the publication year (from the 'gs_a' div)\n",
        "                year = result.find('div', {'class': 'gs_a'})\n",
        "                if year:\n",
        "                    # Split the text and get the last part (usually the year), then strip whitespace\n",
        "                    year = year.text.split('-')[-1].strip()\n",
        "                    article['year'] = year  # Store the year in the dictionary\n",
        "\n",
        "                # Extract the authors (from the 'gs_a' div)\n",
        "                authors = result.find('div', {'class': 'gs_a'})\n",
        "                if authors:\n",
        "                    # Split the text and get the first part (usually the authors), then strip whitespace\n",
        "                    authors = authors.text.split('-')[0].strip()\n",
        "                    article['authors'] = authors  # Store the authors in the dictionary\n",
        "\n",
        "                # Extract the abstract (inside a div element with class 'gs_rs')\n",
        "                abstract = result.find('div', {'class': 'gs_rs'})\n",
        "                if abstract:\n",
        "                    article['abstract'] = abstract.text  # Store the abstract in the dictionary\n",
        "\n",
        "                # Append the article dictionary to the list of articles\n",
        "                articles.append(article)\n",
        "\n",
        "                # Check if the desired number of articles has been collected\n",
        "                if len(articles) >= num_articles:\n",
        "                    return articles\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Main program\n",
        "if __name__ == \"__main__\":\n",
        "    keyword = \"information retrieval\"  # Keyword for the search\n",
        "    start_year = 2013                # Start year of publication range\n",
        "    end_year = 2023                  # End year of publication range\n",
        "    num_articles = 1000              # Desired number of articles to collect\n",
        "\n",
        "    # Call the fetch_google_scholar_articles function to collect articles\n",
        "    articles = fetch_google_scholar_articles(keyword, start_year, end_year, num_articles)\n",
        "\n",
        "    # Save the collected articles to a JSON file\n",
        "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(articles, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "    # Print the number of collected articles and a confirmation message\n",
        "    print(f\"Collected {len(articles)} articles and saved to 'articles.json'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to fetch and display articles from Google Scholar\n",
        "def fetch_and_display_google_scholar_articles(query, start_year, end_year, num_articles):\n",
        "    # Base URL for Google Scholar\n",
        "    url = \"https://scholar.google.com/scholar\"\n",
        "\n",
        "    # Loop to paginate through search results (10 results per page)\n",
        "    for start in range(0, num_articles, 10):\n",
        "        # Parameters for the search query, including keywords and date range\n",
        "        params = {\n",
        "            \"q\": query,             # Search query\n",
        "            \"as_ylo\": start_year,   # Start year of publication range\n",
        "            \"as_yhi\": end_year,     # End year of publication range\n",
        "            \"start\": start          # Pagination offset\n",
        "        }\n",
        "\n",
        "        # User-Agent header to mimic a web browser\n",
        "        header = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.1234.0 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "        # Send a GET request to the Google Scholar search URL with parameters and headers\n",
        "        response = requests.get(url, params=params, headers=headers)\n",
        "\n",
        "        # Check if the response is successful (HTTP status code 200)\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content of the response using BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all the search result div elements\n",
        "            results = soup.find_all('div', {'class': 'gs_ri'})\n",
        "\n",
        "            # Iterate through each search result and display the information\n",
        "            for i, result in enumerate(results, start=1):\n",
        "                print(f\"Article {i}:\")\n",
        "\n",
        "                # Extract the title of the article (inside an h3 element with class 'gs_rt')\n",
        "                title = result.find('h3', {'class': 'gs_rt'})\n",
        "                if title:\n",
        "                    print(f\"Title: {title.text}\")\n",
        "\n",
        "                # Extract the venue/journal/conference information (inside a div element with class 'gs_a')\n",
        "                venue = result.find('div', {'class': 'gs_a'})\n",
        "                if venue:\n",
        "                    print(f\"Venue: {venue.text}\")\n",
        "\n",
        "                # Extract the publication year (from the 'gs_a' div)\n",
        "                year = result.find('div', {'class': 'gs_a'})\n",
        "                if year:\n",
        "                    year = year.text.split('-')[-1].strip()\n",
        "                    print(f\"Year: {year}\")\n",
        "\n",
        "                # Extract the authors (from the 'gs_a' div)\n",
        "                authors = result.find('div', {'class': 'gs_a'})\n",
        "                if authors:\n",
        "                    authors = authors.text.split('-')[0].strip()\n",
        "                    print(f\"Authors: {authors}\")\n",
        "\n",
        "                # Extract the abstract (inside a div element with class 'gs_rs')\n",
        "                abstract = result.find('div', {'class': 'gs_rs'})\n",
        "                if abstract:\n",
        "                    print(f\"Abstract: {abstract.text}\")\n",
        "\n",
        "                print(\"\\n\")  # Add a newline between articles\n",
        "\n",
        "                # Check if the desired number of articles has been displayed\n",
        "                if i >= num_articles:\n",
        "                    return\n",
        "\n",
        "# Main program\n",
        "if __name__ == \"__main__\":\n",
        "    keyword = \"information retrieval\"  # Keyword for the search\n",
        "    start_year = 2013                # Start year of publication range\n",
        "    end_year = 2023                  # End year of publication range\n",
        "    num_articles = 10                # Desired number of articles to display\n",
        "\n",
        "    # Call the fetch_and_display_google_scholar_articles function to display articles\n",
        "    fetch_and_display_google_scholar_articles(keyword, start_year, end_year, num_articles)\n"
      ],
      "metadata": {
        "id": "4Tr3TASD9eQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FymVNKVi9-pb"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import tweepy\n",
        "import json\n",
        "\n",
        "# Set up your Twitter API credentials\n",
        "consumer_key = 'YOUR_CONSUMER_KEY'\n",
        "consumer_secret = 'YOUR_CONSUMER_SECRET'\n",
        "access_token = 'YOUR_ACCESS_TOKEN'\n",
        "access_token_secret = 'YOUR_ACCESS_TOKEN_SECRET'\n",
        "\n",
        "# Authenticate with the Twitter API\n",
        "authenti = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "authenti.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# Creating an API object\n",
        "api = tweepy.API(authenti, wait_on_rate_limit=True)\n",
        "\n",
        "def collect_tweets_by_keyword(keyword, num_tweets):\n",
        "    tweets = []\n",
        "\n",
        "    # Iterate through pages of tweets to collect the desired number\n",
        "    for tweet in tweepy.Cursor(api.search, q=keyword, tweet_mode='extended').items(num_tweets):\n",
        "        tweet_data = {\n",
        "            'User_name': tweet.user.screen_name,  # User's Twitter handle\n",
        "            'Posted_time': tweet.created_at.strftime('%Y-%m-%d %H:%M:%S'),  # Time of the tweet\n",
        "            'Text': tweet.full_text  # The tweet text\n",
        "        }\n",
        "        tweets.append(tweet_data)\n",
        "\n",
        "    return tweets\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    keyword = \"your_keyword_here\"  # Replace with your desired keyword\n",
        "    num_tweets = 1000  # Number of tweets to collect\n",
        "\n",
        "    tweets = collect_tweets_by_keyword(keyword, num_tweets)\n",
        "\n",
        "    # Save the collected tweets to a JSON file\n",
        "    with open(\"tweets.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(tweets, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Collected {len(tweets)} tweets and saved to 'tweets.json'.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}